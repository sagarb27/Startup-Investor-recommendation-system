{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_Crunchbase_data_collection.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ky7XJoY1yDC9","colab_type":"text"},"source":["## Following code reads crunchbase web link for each startup from 'startup.csv' and fetches the startup's information from crunchbase.com. It then writes the info in 'startup2.csv'. Because crunchbase.com does not allow webscraping we cannot hit requests continuously. So a random delay is added after every request."]},{"cell_type":"markdown","metadata":{"id":"3G5MbwrsyDC_","colab_type":"text"},"source":["## Even after putting the delay, requests get blocked after some time. In this case if we run the main block again then the data gathering starts after our last successfully attempt. That is the program checks which startups' data has already been collected and starts hitting requests for startups after that point."]},{"cell_type":"code","metadata":{"id":"HfVDzC90yDDA","colab_type":"code","colab":{}},"source":["import requests\n","import urllib.request\n","import time\n","from bs4 import BeautifulSoup\n","import csv\n","import random\n","from urllib.request import urlopen, Request"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RslBtKPiyDDD","colab_type":"code","colab":{}},"source":["def getSpanTagData(soup,fieldName):\n","    span_tags = soup.findAll(\"span\")\n","    i=0\n","    found = False\n","    while i < len(span_tags):\n","        if (span_tags[i].get_text() == fieldName):\n","            #print(span_tags[i])\n","            found = True\n","            break\n","        i+=1\n","    i+=3\n","    if found == True:\n","        return span_tags[i].get_text()\n","    else:\n","        return \"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzja8QyOyDDG","colab_type":"code","colab":{}},"source":["def findAlreadyFoundCount():\n","    with open('D:\\\\cmpe256\\\\Team project\\\\startups2.csv','r') as file1:\n","        csvreader = csv.reader(file1)\n","\n","        row = csvreader.__next__()\n","        already_found_count = sum(1 for row in csvreader)\n","        #print(already_found_count)\n","    file1.close()\n","    return already_found_count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DL8kkdDUyDDJ","colab_type":"text"},"source":["### Main block to run for getting cruchbase data"]},{"cell_type":"code","metadata":{"id":"2qGsZP47yDDK","colab_type":"code","colab":{}},"source":["with open('D:\\\\cmpe256\\\\Team project\\\\startups.csv','r') as file1:\n","    csvreader = csv.reader(file1)\n","\n","    row = csvreader.__next__()\n","    skip_count = findAlreadyFoundCount()\n","    while skip_count > 0:\n","        row = csvreader.__next__()\n","        skip_count-=1\n","\n","    count = 1\n","    useragent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n","    headers = {'User-Agent' : useragent}\n","\n","    with open('D:\\\\cmpe256\\\\Team project\\\\startups2.csv','a',newline=\"\") as file2:\n","        writer = csv.writer(file2)\n","        \"\"\"writer.writerow(['Name','State','Crunchbase link','Angel Link','Website','Start date','Exit Value',\n","                             'Unknown','Funding','Categories','Region','IPO Status','Last Funding Type',\n","                             'Number of Funding Rounds','Number of Articles','Number of Employees'])\"\"\"\n","\n","\n","        for row in csvreader:\n","            print (row[0])\n","            if 'crunchbase' in row[2]:\n","                url=row[2]\n","                req = urllib.request.Request(url,headers=headers)\n","                with urllib.request.urlopen(req) as response:\n","                    startup_page = response.read()\n","                    soup = BeautifulSoup(startup_page, \"html.parser\")\n","                    span_tags = soup.findAll(\"span\")\n","\n","                    categories = getSpanTagData(soup,\"Categories\")\n","                    region = getSpanTagData(soup,\"Headquarters Regions\")\n","                    ipostatus = getSpanTagData(soup,\"IPO Status\")\n","                    lastfundtype = getSpanTagData(soup,\"Last Funding Type\")\n","                    fundingRounds = getSpanTagData(soup,\"Number of Funding Rounds\")\n","                    articles = getSpanTagData(soup,\"Number of Articles\")\n","                    employees = getSpanTagData(soup,\"Number of Employees\")\n","                    employees = \" \" + employees\n","                    investors_count = getSpanTagData(soup,\"Number of Investors\")\n","                    info = [categories,region,ipostatus,lastfundtype,fundingRounds,articles,employees,investors_count] \n","                    print([categories,region,ipostatus,lastfundtype,fundingRounds,articles,employees,investors_count])\n","\n","                    for field in info:\n","                        row.append(field)\n","            writer.writerow(row)\n","            time.sleep(random.randint(41,94))"],"execution_count":0,"outputs":[]}]}