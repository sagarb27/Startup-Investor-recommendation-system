{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Angel_data_collection.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yKP9_9PKwJDP","colab_type":"text"},"source":["# Get data from angel.co and put in csv. Angel pages saved on local and then accessed."]},{"cell_type":"code","metadata":{"id":"BLNVgqmXwJDR","colab_type":"code","colab":{}},"source":["import requests\n","import urllib.request\n","import time\n","from bs4 import BeautifulSoup\n","import csv\n","import random\n","from urllib.request import urlopen, Request\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJGBanKYwJDV","colab_type":"text"},"source":["### Find the html file of the given startup_name in the directory. Function returns the beautifulsoup object of that file"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-I3CJXV7wJDW","colab_type":"code","colab":{}},"source":["def find_file(startup_name):\n","    path = \"D:\\\\cmpe256\\\\Team project\\\\angel\\\\\"\n","    path2 = \"D:\\\\cmpe256\\\\Team project\\\\angel2\\\\\"\n","    filesList = os.listdir(path)\n","    for file_name in filesList:\n","        filePath = path + file_name\n","        #print(filePath)\n","        file = open(filePath,'rb')\n","        soup = BeautifulSoup(file.read(), \"html.parser\")\n","        name = soup.find('div',class_='summary').find('h1').text.replace('\\n','')\n","        #print(name)\n","        if name.lower().strip() == startup_name.lower().strip():\n","            file.close()\n","            os.rename(filePath, path2+file_name)\n","            return soup\n","        file.close()\n","    return None\n","find_file('hi')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7plwS_HwJDY","colab_type":"text"},"source":["### The following 2 functions gets the funding rounds and investor information from the html and returns them in the form of string."]},{"cell_type":"code","metadata":{"id":"fvji1cTkwJDZ","colab_type":"code","colab":{}},"source":["def get_investors(soup):\n","    div_tags = soup.findAll(\"div\",attrs={\"class\": \"group\"})\n","\n","    investor_div_tag = None\n","    for one_div_tag in div_tags:\n","        investor_div_tag = one_div_tag.find(\"div\",attrs={\"data-role\":\"past_investor\"})\n","        if investor_div_tag != None:\n","            break\n","    li_tags = []\n","    if investor_div_tag != None:\n","        li_tags = investor_div_tag.findAll(\"li\")\n","    investors_list = \"\"\n","    for one_tag in li_tags:\n","        name = \"\"\n","        role_title = \"\"\n","        bio = \"\"\n","        if one_tag.find('div',class_='name') != None:\n","            name = one_tag.find('div',class_='name').text\n","        if one_tag.find('div',class_='role_title') != None:\n","            role_title = one_tag.find('div',class_='role_title').text\n","        if one_tag.find('div',class_='bio') != None:\n","            bio = one_tag.find('div',class_='bio').text\n","        name = name.replace('\\n','')\n","        #print(name + '|' + role_title + '|' + bio)\n","        investors_list = investors_list + '[' + name + '|' + role_title + '|' + bio + ']'\n","    return investors_list.encode(\"ascii\",errors=\"ignore\").decode()\n","\n","def get_funding_rounds(soup):\n","    investment_div_tag = soup.find(\"div\",class_=\" dsss17 startups-show-sections fss49 startup_rounds _a _jm\")\n","    li_tags = []\n","    if investment_div_tag != None:\n","        li_tags = investment_div_tag.findAll(\"li\")\n","    funding_list = \"\"\n","    for one_tag in li_tags:\n","        #print(one_tag.text)\n","        fund_type = one_tag.find(\"div\",class_=\"type\").text\n","        money = one_tag.find(\"div\", class_=\"raised\").text\n","        participants_list = []\n","        participants_tag = []\n","        if one_tag.find(\"div\",class_=\"participant_list inner_section\") != None:\n","            participants_tag = one_tag.find(\"div\",class_=\"participant_list inner_section\").findAll(\"div\",class_=\"participant g-lockup\")\n","        for single_participant in participants_tag:\n","            participants_list.append(single_participant.find(\"div\",class_=\"name\").text.replace('\\n',''))\n","            #print(single_participant.find(\"div\",class_=\"name\").text)\n","        fund_type = fund_type.replace(\"\\n\",\"\")\n","        money = money.replace(\"\\n\",\"\")\n","        #print(fund_type + '|' + money + '|' + str(participants_list))\n","        funding_list = funding_list + '[' + fund_type + '|' + money + '|' + str(participants_list) + ']'\n","    return funding_list.encode(\"ascii\",errors=\"ignore\").decode()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFZZNTaVwJDc","colab_type":"text"},"source":["### Main code to run. This code scans every entry from the first csv, searches its html in the directory, gets the funding and investor data and writes it in the second csv."]},{"cell_type":"code","metadata":{"id":"Me-hUAi2wJDc","colab_type":"code","outputId":"3d6e0775-407f-402e-daac-628bbc0c855b","colab":{}},"source":["with open('D:\\\\cmpe256\\\\Team project\\\\startups2.csv','r') as file1:\n","    csvreader = csv.reader(file1)\n","\n","    row = csvreader.__next__()\n","    with open('D:\\\\cmpe256\\\\Team project\\\\startups3.csv','w',newline=\"\") as file2:\n","        writer = csv.writer(file2)\n","        writer.writerow(['Name','State','Crunchbase link','Angel Link','Website','Start date','Exit Value',\n","                         'Unknown','Funding','Categories','Region','IPO Status','Last Funding Type',\n","                         'Number of Funding Rounds','Number of Articles','Number of Employees',\n","                         'Number of Investors','Investors','Funding rounds'])\n","        for row in csvreader:\n","            soup = find_file(row[0])\n","            if soup != None:\n","                print(\"Got match\")\n","                investors_list = get_investors(soup)\n","                funding_list = get_funding_rounds(soup)\n","                row.append(investors_list)\n","                row.append(funding_list)\n","                #print(row)\n","            #print(row)\n","            writer.writerow(row)\n","    file2.close()\n","file1.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n","Got match\n"],"name":"stdout"}]}]}